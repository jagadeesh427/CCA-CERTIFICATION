
sqoop import

hive

importing into already existing table in hive

sqoop import 
--connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" 
--username retail_dba 
--password itversity 
--table departments 
--hive-home /apps/hive/warehouse 
--hive-import 
--hive-overwrite 
--hive-table sqoop_importj.departments 
--outdir java_files

creating hive table while importing

sqoop import 
--connect "jdbc:mysql://nn01.itversity.com/retail_db" 
--username retail_dba
--password itversity 
--table departments 
--hive-home /apps/hive/warehouse 
--hive-import 
--hive-table sqoop_importj.departments_test 
--create-hive-table 
--outdir java_files






sqoop delimiters

using enclosedby

sqoop import
--connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" 
--username retail_dba 
--password itversity 
--table departments 
--target-dir /user/jagadeesh427/sqoop_import/departments_enclosedby 
--enclosed-by \"



sqoop import 
--connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" 
--username retail_dba 
--password itversity 
--table departments 
--target-dir /user/jagadeesh427/sqoop_import/departments_enclosedby 
--enclosed-by \" 
--fields-terminated-by \| 
--lines-terminated-by \|



hadoop fs -cat /user/jagadeesh427/sqoop_import/departments_enclosedby/part*


sqoop import 
--connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" 
--username retail_dba 
--password itversity 
--table departments 
--hive-home /apps/hive/warehouse 
--hive-import 
--hive-table departments_test_01 
--create-hive-table
--outdir java_files




null string::;


--null-string 
--null-non-string 



export delimiters:



file format

sqoop import 
--connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" 
--username retail_dba 
--password itversity 
--table departments 
--as-sequencefile 
--target-dir=/user/jagadeesh427/departments

hadoop fs -cat /user/jagadeesh427/departments/part*

sqoop import 
--connect "jdbc:mysql://nn01.itversity.com:3306/retail_db" 
--username retail_dba 
--password itversity 
--table departments 
--as-avrodatafile 
--target-dir=/user/jagadeesh427/departments


hadoop fs -cat /user/jagadeesh427/departments/part*






sqoop job and sqoop merge


sqoop job 
--create sqoop_job 
-- import --connect "jdbc:mysql://nn01.itversity.com:3306/retail_import" 
--username retail_dba
--password itversity 
--table departments 
--target-dir /apps/hive/warehouse/retail_ods.db/departments 
--append 
--fields-terminated-by '|'
--lines-terminated-by '\n' 
--check-column "department_id"
--incremental append 
--last-value 7
--outdir java_files

sqoop job --list

sqoop job --show sqoop_job


sqoop job --exec sqoop_job


sqoop import 
--connect "jdbc:mysql://nn01.itversity.com:3306/retail_import"
--username retail_dba 
--password itversity 
--table departments 
--target-dir=/user/jagadeesh427/sqoop_merge/departments



sqoop eval 
--connect "jdbc:mysql://nn01.itversity.com:3306/retail_import" 
--username retail_dba  
--password itversity 
--query "select * from departments"


hadoop fs -cat /user/jagadeesh427/sqoop_merge/departments/part*


sqoop eval
--connect "jdbc:mysql://nn01.itversity.com:3306/retail_import" 
--username retail_dba  
--password itversity 
--query "update departments set department_name='Testing Merge_01' where department_id = 9000"




sqoop eval
--connect "jdbc:mysql://nn01.itversity.com:3306/retail_import"
--username retail_dba  
--password itversity 
--query "insert into departments values (10000, 'inserting for merge1')"




sqoop eval 
--connect "jdbc:mysql://nn01.itversity.com:3306/retail_import" 
--username retail_dba  
--password itversity
--query "select * from departments"





















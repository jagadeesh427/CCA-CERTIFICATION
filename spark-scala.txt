val departmentsRDD = sc.textFile("hdfs://nn01.itversity.com:8020/user/jagadeesh427/sqoop_import/departments")

departmentsRDD.collect()

val departmetnRDD = sc.textFile("/user/jagadeesh427/sqoop_import/departments")
departmetnRDD.collect().foreach(println)


dataRDD.map(x => (NullWritable.get(), x)).saveAsSequenceFile("/user/jagadeesh427/sparkscala/departmentseq")

#reading a seqenceFile

sc.sequenceFile("/user/jagadeesh427/sparkscala/departmentseq",classOf[IntWritable], classOf[Text]).map(rec => rec.toString()).collect().foreach(println)


import org.apache.spark.sql.hive.HiveContext
val sqlContext = new HiveContext(sc)
val deptsRDD = sqlContext.sql("select * from departments")

failed

JSON
create json file departmetns.json

"department_id":2, "department_name":"Fitness"}
{"department_id":3, "department_name":"Footwear"}
{"department_id":4, "department_name":"Apparel"}
{"department_id":5, "department_name":"Golf"}
{"department_id":6, "department_name":"Outdoors"}
{"department_id":7, "department_name":"Fan Shop"}
{"department_id":8, "department_name":"TESTING"}




import org.apache.spark.sql.SQLContext
val sqlContext = new SQLContext(sc)

val departmentsJSON = sqlContext.jsonFile("/user/jagadeesh427/sparkscala/departments.json")
departmentsJSON.registerTempTable("departmentsTable")
val departmentsData = sqlContext.sql("select * from departmentsTable")
departmentsData.collect().foreach(println)

# writing  json data into text

departmentsData.toJSON.saveAsTextFile("/user/jagadeesh427/sparkscala/departmentsJson")

hadoop fs -cat /user/jagadeesh427/sparkscala/departmentsJson/part*




#WORDCOUNT

val dataRDD = sc.textFile("/user/jagadeesh427/wordcount")
val dataFlatmap = dataRDD.flatMap(x => x.split(" "))
val dataMap = dataFlatmap.map(x =>  (x,1))
val dataReduceByKey = dataMap.reduceByKey((x,y) => x = y)
dataReduceByKey.saveAsTextFile("/user/jagadeesh427/sparkWCOUTPUT.txt")


# Joining datasets

val ordersRDD = sc.textFile("/user/jagadeesh427/sqoop_import/orders")
ordersRDD.take(5).foreach(println)
val orderitemsRDD = sc.textFile("/user/jagadeesh427/sqoop_import/order_items")
orderitemsRDD.take(5).foreach(println)

 val ordersparsedRDD = ordersRDD.map(rec  => (rec.split(",")(0).toInt, rec))

val orderitemsparsedRDD = orderitemsRDD.map(rec => (rec.split(",")(1).toInt, rec))
ordersjoinorderitems.take(5).foreach(println)

val revenueperorderperday = ordersjoinorderitems.map(t => (t._2._2.split(",")(1), t._2._1.split(",")(4).toFloat))


# order count per day
val ordersperday = ordersjoinorderitems.map(rec => rec._2._2.split(",")(1) + "," + rec._1).distinct()
val ordersperdayparsedRDD = ordersperday.map(rec => (rec.split(",")(0),1))

val totalordersperday = ordersperdayparsedRDD.reduceByKey((x, y) => x + y)

#total revenue per day from joined data

val revenueperorderperday = ordersjoinorderitems.map(t => (t._2._2.split(",")(1), t._2._1.split(",")(4).toFloat))
val totalrevenueperday = revenueperorderperday.reduceByKey((total1, total2) => total1 + total2)
totalrevenueperday.sortByKey().collect().foreach(println)

#joining ordercount per day and revenue perday
val finaljoinRDD = totalordersperday.join(totalrevenueperday)




# Using Hive


import org.apache.spark.sql.hive.HiveContext
val sqlContext = new HiveContext(sc)
sqlContext.sql("set spark.sql.shuffle.partitions=10");

val joinAggData = sqlContext.sql("select o.order_date, round(sum(oi.order_item_subtotal), 2), count(distinct o.order_id) from orders o join order_items oi  on o.order_id = oi.order_item_order_id group by o.order_date order by o.order_date")


joinAggData.collect().foreach(println)


#AGGREGATING

#total revenue

 val orderItemsRDD = sc.textFile("/user/jagadeesh427/sqoop_import/order_items")
val orderItmesMap = orderItemsRDD.map(rec  => (rec.split(",")(4).toDouble))
val orderItemsReduce = orderItmesMap.reduce((x, y) => x + y )
printf("%f", orderItemsReduce)


# AVERAGE REVNUE
val ordcount = orderItemsRDD.map(_.split(",")(1).toInt).distinct().count()
orderItemsReduce/ordcount

#Get Max PRICE OF the Product


val oderitemsmap = orderItemsRDD.map(rec => (rec.split(",")(1),rec.split(",")(4)))

val revperorder = oderitemsmap.reduceByKey((x,y) => x + y)

revperorder.reduce((acc, value) =>
(if (acc._2 >= value._2) acc
else value))































































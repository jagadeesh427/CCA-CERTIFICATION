pyspark


from pyspark.sql  import HiveContext
sqlContext = HiveContext(sc)
depts = sqlContext.sql("select * from categories")


for rec in depts.collect():
   print(rec)
   
   
   
  create file saveFile.py
  
  from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName("pyspark")
sc = SparkContext(conf=conf)
dataRDD = sc.textFile("/user/jagadeesh427/sqoop_import/departments")
for line in dataRDD.collect():
   print(line)
dataRDD.saveAsTextFile("/user/jagadeesh427/pyspark/departments")


spark-submit --master yarn or local saveFile.py


# Load data form HDFS and storing results back from HDFS using Spark



sc.textFile("/user/jagadeesh427/sqoop_import/departments")

data = sc.textFile("/user/jagadeesh427/sqoop_import/departments")

to read and print

for i in data.collect():
   print(i)
   
#READING File from local 

data = sc.textFile(file:///home/jagadeesh427/demo)


#Reading data from HIve and JSON

from pyspark.sql import HiveContext
sqlContext = HiveContext(sc)
data = sqlContext.sql(" select * from departments_1")

for i in data.collect():
  print(i)
  
  
  data = sqlContext.sql("create table departmentstestj as select * from departments_1")


#WORDCOUNT USING PYSPARK

create text file in local system

data = sc.textFile("/user/jagadeesh427/wordcount1.txt")
dataFlatMap = data.flatMap(lambda x: x.split(" "))
dataMap = dataFlatMap.map(lambda x: (x,1))
wordcount = dataMap.reduceByKey(lambda x,y: x + y)
wordcount.saveAsTextFile("/user/jagadeesh427/wordcountoutput.com")


#Joining datasets using PYTHON


ordersRDD = sc.textFile("/user/jagadeesh427/sqoop_import/orders")
orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")


ordersParsedRDD = ordersRDD.map(lambda rec: (int(rec.split(",")[0]), rec))
orderItemsParsedRDD = orderItemsRDD.map(lambda rec: (int(rec.split(",")[1]), rec))

 ordersJoinOrderItems = orderItemsParsedRDD.join(ordersParsedRDD)
 
 joinData = ordersJoinOrderItems.map(lambda t: (t[1][1].split(",")[1], float(t[1][0].split(",")[4])))
 
 


#joining datasets using SQL context

from pyspark.sql import HiveContext
sqlContext = HiveContext(sc)
count = sqlContext.sql("select count(1) from orders")

for i in count.collect():
  print(i)
  
 count = sqlContext.sql("select count(1) from order_itmes") 
 
 for i in count.collect():
   print(i)
   
   
joinRDD = sqlContext.sql("select o.order_date, sum(oi.order_item_subtotal), 
count(distinct o.order_id) from orders o join order_items oi on o.order_id = oi.order_item_order_id \  

group by o.order_date order by o.order_date")



#Calculate aggregate statistics





































pyspark


from pyspark.sql  import HiveContext
sqlContext = HiveContext(sc)
depts = sqlContext.sql("select * from categories")


for rec in depts.collect():
   print(rec)
   
   
   
  create file saveFile.py
  
  from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName("pyspark")
sc = SparkContext(conf=conf)
dataRDD = sc.textFile("/user/jagadeesh427/sqoop_import/departments")
for line in dataRDD.collect():
   print(line)
dataRDD.saveAsTextFile("/user/jagadeesh427/pyspark/departments")


spark-submit --master yarn or local saveFile.py


# Load data form HDFS and storing results back from HDFS using Spark



sc.textFile("/user/jagadeesh427/sqoop_import/departments")

data = sc.textFile("/user/jagadeesh427/sqoop_import/departments")

to read and print

for i in data.collect():
   print(i)
   
#READING File from local 

data = sc.textFile(file:///home/jagadeesh427/demo)


#Reading data from HIve and JSON

from pyspark.sql import HiveContext
sqlContext = HiveContext(sc)
data = sqlContext.sql(" select * from departments_1")

for i in data.collect():
  print(i)
  
  
  data = sqlContext.sql("create table departmentstestj as select * from departments_1")


#WORDCOUNT USING PYSPARK

create text file in local system

data = sc.textFile("/user/jagadeesh427/wordcount1.txt")
dataFlatMap = data.flatMap(lambda x: x.split(" "))
dataMap = dataFlatMap.map(lambda x: (x,1))
wordcount = dataMap.reduceByKey(lambda x,y: x + y)
wordcount.saveAsTextFile("/user/jagadeesh427/wordcountoutput.com")


#Joining datasets using PYTHON


ordersRDD = sc.textFile("/user/jagadeesh427/sqoop_import/orders")
orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")


ordersParsedRDD = ordersRDD.map(lambda rec: (int(rec.split(",")[0]), rec))
orderItemsParsedRDD = orderItemsRDD.map(lambda rec: (int(rec.split(",")[1]), rec))

 ordersJoinOrderItems = orderItemsParsedRDD.join(ordersParsedRDD)
 
 joinData = ordersJoinOrderItems.map(lambda t: (t[1][1].split(",")[1], float(t[1][0].split(",")[4])))
 
 


#joining datasets using SQL context

from pyspark.sql import HiveContext
sqlContext = HiveContext(sc)
count = sqlContext.sql("select count(1) from orders")

for i in count.collect():
  print(i)
  
 count = sqlContext.sql("select count(1) from order_itmes") 
 
 for i in count.collect():
   print(i)
   
   
joinRDD = sqlContext.sql("select o.order_date, sum(oi.order_item_subtotal), 
count(distinct o.order_id) from orders o join order_items oi on o.order_id = oi.order_item_order_id \  

group by o.order_date order by o.order_date")



#Calculate aggregate statistics


ordersRDD = sc.textFile("/user/jagadeesh427/sqoop_import/orders")
ordersRDD.count()
orderItemsRDD = sc.textFile("/user/jagadeesh427/sqoop_import/order_items")
orderItemsRDDMap = orderItemsRDD.map(lambda rec: rec.split(",")[4])
for i in orderItemsRDDMap.take(5):


orderItemsRDDMap = orderItemsRDD.map(lambda rec: float(rec.split(",")[4]))
orderItemsRDDMap.reduce(lambda rev1, rev2: rev1+rev2)

# max price of product from product table
productRDD = sc.textFile("/user/jagadeesh427/sqoop_import/products")

productMap = productsRDD.map(lambda rec: rec)


productMap.reduce(lambda rec1, rec2: (rec1 if((rec1.split(",")[4] != "" and rec2.split(",")[4] != "") 
and float(rec1.split(",")[4]) >= float(rec2.split(",")[4])) else rec2))


#average


orderitemsRDD = sc.textFile("/user/jagadeesh427/sqoop_import/order_items")
totalrevenue = orderitemsRDD.map(lambda rec: float(rec.split(",")[4]).reduce(lambda acc, val: acc+val))
totalrevenue

or
totalrevenue = orderitemsRDD.map(lambda rec: float(rec.split(",")[4]))

totalR = totalrevenue.reduce(lambda acc, val: acc+val)
totalorders = orderitemsRDD.map(lambda rec: rec.split(",")[1]).distinct().count()

totalR/totalorders








#FLITERING using PYTHON

ordersRDD = sc.textFile("/user/jagadeesh427/sqoop_import/orders")
ordersRDDF = ordersRDD.filter(lambda line: line.split(",")[3] == "COMPLETE")
for i in ordersRDDF.take(5): print(i)


for i in ordersRDD.filter(lambda line: line.split(",")[3] == "COMPLETE").take(5): print(i)

for i in ordersRDD.filter(lambda line: "PENDING" in line.split(",")[3]).take(5): print(i)

for i in ordersRDD.filter(lambda line: int(line.split(",")[0]) > 100).take(5): print(i)
 
for i in ordersRDD.filter(lambda line: int(line.split(",")[0]) > 100 or line.split(",")[3] in "PENDING").take(5): print(i)
 
for i in ordersRDD.filter(lambda line: int(line.split(",")[0]) > 1000 and ("PENDING" in line.split(",")[3] or line.split(",")[3] == ("CANCELLED"))).take(5): print(i)
 
for i in ordersRDD.filter(lambda line: int(line.split(",")[0]) > 1000 and line.split(",")[3] != ("COMPLETE")).take(5): print(i)


#check if there are any canceled orders above $1000

ordersRDD = sc.textFile("/user/jagadeesh427/sqoop_import/orders")
orderItemsRDD = sc.textFile("/user/jagadeesh427/sqoop_import/order_items")


ordersParsedRDD = ordersRDD.filter(lambda rec: rec.split(",")[3] in "CANCELED").map(lambda rec: (int(rec.split(",")[0]), rec))
orderItemsParsedRDD = orderItemsRDD.map(lambda rec: (int(rec.split(",")[1]), float(rec.split(",")[4])))
orderItemsAgg = orderItemsParsedRDD.reduceByKey(lambda acc, value: (acc + value))

ordersJoinOrderItems = orderItemsAgg.join(ordersParsedRDD)

for i in ordersJoinOrderItems.filter(lambda rec: rec[1][0] >= 1000).take(5): print(i)



#Write a query that produces ranked or sorted data using Spark
























pyspark


from pyspark.sql  import HiveContext
sqlContext = HiveContext(sc)
depts = sqlContext.sql("select * from categories")


for rec in depts.collect():
   print(rec)
   
   
   
  create file saveFile.py
  
  from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName("pyspark")
sc = SparkContext(conf=conf)
dataRDD = sc.textFile("/user/jagadeesh427/sqoop_import/departments")
for line in dataRDD.collect():
   print(line)
dataRDD.saveAsTextFile("/user/jagadeesh427/pyspark/departments")


spark-submit --master yarn or local saveFile.py


# Load data form HDFS and storing results back from HDFS using Spark



sc.textFile("/user/jagadeesh427/sqoop_import/departments")

data = sc.textFile("/user/jagadeesh427/sqoop_import/departments")

to read and print

for i in data.collect():
   print(i)
   
#READING File from local 

data = sc.textFile(file:///home/jagadeesh427/demo)


#Reading data from HIve and JSON

from pyspark.sql import HiveContext
sqlContext = HiveContext(sc)
data = sqlContext.sql(" select * from departments_1")

for i in data.collect():
  print(i)
  
  
  data = sqlContext.sql("create table departmentstestj as select * from departments_1")


#WORDCOUNT USING PYSPARK

create text file in local system

data = sc.textFile("/user/jagadeesh427/wordcount1.txt")
dataFlatMap = data.flatMap(lambda x: x.split(" "))
dataMap = dataFlatMap.map(lambda x: (x,1))
wordcount = dataMap.reduceByKey(lambda x,y: x + y)
wordcount.saveAsTextFile("/user/jagadeesh427/wordcountoutput.com")










































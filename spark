pyspark


from pyspark.sql  import HiveContext
sqlContext = HiveContext(sc)
depts = sqlContext.sql("select * from categories")


for rec in depts.collect():
   print(rec)
   
   
   
  create file saveFile.py
  
  from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName("pyspark")
sc = SparkContext(conf=conf)
dataRDD = sc.textFile("/user/jagadeesh427/sqoop_import/departments")
for line in dataRDD.collect():
   print(line)
dataRDD.saveAsTextFile("/user/jagadeesh427/pyspark/departments")


spark-submit --master yarn or local saveFile.py


# Load data form HDFS and storing results back from HDFS using Spark



sc.textFile("/user/jagadeesh427/sqoop_import/departments")

data = sc.textFile("/user/jagadeesh427/sqoop_import/departments")

to read and print

for i in data.collect():
   print(i)
   
#READING File from local 

data = sc.textFile(file:///home/jagadeesh427/demo)













